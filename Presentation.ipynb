{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minor Project Presentation\n",
    "\n",
    "**Presented by:**\n",
    "\n",
    "<ol>\n",
    "    <li>Krutika Bapat 16100034</li>\n",
    "    <li>Kushashwa Ravi Shrimali 16100035</li>\n",
    "    <li>Saurabh Kumar Singh 16100051</li>\n",
    "</ol>\n",
    "    \n",
    "**Guide:**\n",
    "\n",
    "<ol>\n",
    "    <ul>Dr. Vivek Tiwari (Assistant Professor, Computer Science and Engineering, IIIT NR)</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Project Title \n",
    "\n",
    "**Text to Image Synthesis using Generative Adversarial Networks for Complex Features** \n",
    "\n",
    "The target is to synthesize images using Text from the user, using Generative Adversarial Networks (type of Generative Models with Discriminator Network added). The added target is to train the Machine to learn to synthesize images with complex features. \n",
    "\n",
    "<img src=\"images/gan_sample.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notable works** for this task include: $\\text{StackGANs[2]}$, $\\text{StackGAN++[3]}$, $\\text{ReedScott ICML 2016[4]}$, $\\text{GAN INT-CLS[5]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are GANs?\n",
    "\n",
    "Generative Adversarial Networks[1], first proposed in 2014 can generate synthetic images better than previous generative models. \n",
    "\n",
    "There has been several research in GANs and modifications in the existing approach include: <strong>WGANs, DCGANs, LAPGANs, STACKGANs, SGANs, cGANs</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use of Text to Image Synthesis? \n",
    "\n",
    "It helps to control the content of generated images. It is a recent challenge to generate images based on text descriptions. \n",
    "\n",
    "<center><img src=\"images/txt2images.png\">Source: https://arxiv.org/pdf/1605.05396v2.pdf</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ol>\n",
    "<li>One of the most popular usage of Text to Image using GANs include the development of an app for a company to give users a visual preview of *Sarees* based on their own description.</li>\n",
    "\n",
    "<li>Other possible applications include, generating faces of criminals and missing people based on their features given by the persons.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation\n",
    "\n",
    "We were inspired from StackGAN++ and ReedScoot ICML 2016 Paper[4] and it's code[6]. The implementation requires following libraries to be installed:\n",
    "\n",
    "<ol>\n",
    "    <li>Torch (Lua)</li>\n",
    "    <li>CuDNN</li>\n",
    "    <li>Display Package</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why CuDNN?\n",
    "\n",
    "<center><img src=\"images/cudnn_performance.png\">Source: https://developer.nvidia.com/cudnn</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network Description (StackGAN++)\n",
    "\n",
    "<img src=\"images/stackganpp.png\">Source: https://github.com/hanzhanggit/StackGAN-v2</img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## StackGAN++ vs StackGAN\n",
    "\n",
    "This will be added later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<ol>\n",
    "    <li>PyTorch Implementation of StackGAN++: https://github.com/hanzhanggit/StackGAN-v2</li>\n",
    "    <li>Tensorflow Implementation of StackGAN: https://github.com/hanzhanggit/StackGAN</li>\n",
    "</ol>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before Mid-Sem vs Now\n",
    "\n",
    "**Progress?**\n",
    "\n",
    "Before Mid-Term, we were able to train the machine using StackGAN network and reproduce the results for Flowers[6] and Birds[7] Dataset. While this remained a challenge for us to train on GPU, with presence of GPU Lab, we were able to now train for:\n",
    "\n",
    "<ol>\n",
    "    <li>MS COCO Dataset: <strong>link here</strong></li>\n",
    "    <li>LSUN Dataset: <strong>link here</strong></li>\n",
    "</ol>\n",
    "\n",
    "Apart from this, we were also able to successfully reproduce results of ReedScott's Paper on Text to Image Synthesis, presented in ICML 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We were also able to think of a novel idea to approach, for our future changes. \n",
    "\n",
    "Geoffrey Hinton, pioneer in Deep Learning, responsible for contributions to Deep Learning and AI, like Back-Propagation[8], and Error Propagation[9], contributed the society with **Capsule Networks**[10] idea.\n",
    "\n",
    "We give a brief review on their usage and dos and donts."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
